{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from calendar import monthrange\n",
    "import re\n",
    "plt.style.use('dark_background')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 of 4\n",
    "\n",
    "Given the files in then mox_technical_assessment folder:\n",
    "\n",
    "Questions\n",
    "\n",
    "## 1. Show us how you would structure them (and why). \n",
    "I start by opening them an inspecting their structure in json.\n",
    "After inspection of the json file I guessed that is was a case of structure data with nested parameters.\n",
    "I will start by flattening the json file which automatically joins all the keys by '\\_' as column name.\n",
    "Ending up with tabular data that can be stored in a pandas DataFrame. and easily exported to other applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mox_technical_assessment/customer-events.json', 'r') as f:\n",
    "    ce = json.load(f) # Load raw jason data as a dictionary\n",
    "\n",
    "with open('mox_technical_assessment/transaction-events.json', 'r') as f:\n",
    "    te = json.load(f) # Load raw jason data as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten customer jason records \n",
    "df_ce = pd.json_normalize(ce, sep='_')\n",
    "# Flatten trx jason records\n",
    "df_te = pd.json_normalize(te, sep='_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How you would name the variables and tables (and why)\n",
    "Looking closer at the structure I decide to change a bit the column names:\n",
    "* remove payload since it is not adding information to the field. \n",
    "* \"Detail\" actually refers to _customer_ detail so I also change it to \"customer\"\n",
    "* I also choose to change camel-case to snake-case: customerID -> customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ce.columns = [c.replace('detail', 'customer').replace('payload_', '') for c in df_ce.columns]\n",
    "df_ce = df_ce.rename(columns={'customerId': 'customer_id'})\n",
    "\n",
    "df_te.columns = [c.replace('payload_', '') for c in df_te.columns]\n",
    "df_te = df_te.rename(columns={'customerId': 'customer_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now cast the types since many of the fields are stored as string but are actually int of float. \n",
    "I also check that the customer id is unique in the customer event table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For customer set index to customer_id and assert there are no duplicates\n",
    "# Cast age, salary to int and float\n",
    "df_ce['customer_age'] = df_ce.customer_age.astype(int)\n",
    "df_ce['customer_salary'] = df_ce.customer_salary.astype(float)\n",
    "assert df_ce.index.duplicated().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the timestamp as such\n",
    "df_te['timestamp'] = pd.to_datetime(df_te.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit more complicated is the description field of the transaction events table. Upon inspection there seems to be a relatively easy way of obtaining the category of the event which can be very useful.\n",
    "\n",
    "To extract it i use a regular expresion that will capture the last item in parenthesis in a line that does not include the character \"(\" in it. This is because some cases are like this: ```something (wan chai (rent)```\n",
    "I looked at the resulting categories and it seems correct. However this is a risk in cases where the data changes since we don't know if the regex will work everytime. We would need to add checks no inform us of any new categories that might be a product of a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find last term in parenthesis in description, exclude matches that include a '('\n",
    "df_te['category'] = df_te.description.apply(lambda s: re.findall('^.*\\(([^\\(]*?)\\)$',s)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally since we have HKD and USD trx, we will need a table to convert, ideally we would call and API to obtain the exact conversion rate and the date of the event but since the HKD is pegged we can assume 7.75 without much problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For aggregates create a converted currency ammount column, I chose HKD but it could be USD too\n",
    "df_curr = pd.DataFrame([['HKD', 1.], ['USD', 7.75]], columns=['transaction_currency', 'conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "3. Download Tableau Desktop (Free Trial) from this link here, explore the data in detail using the\n",
    "client\n",
    "4. Walk us through what you’ve built, and tell us what type of patterns have you observed within\n",
    "the data (including any anti-patterns or issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te.to_csv('files_for_tableau/transactions_events.csv', index=False)\n",
    "df_ce.to_csv('files_for_tableau/customer_events.csv', index=False)\n",
    "df_curr.to_csv('files_for_tableau/currency_exchange.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Download Tableau Desktop (Free Trial) from this link here, explore the data in detail using the client\n",
    "\n",
    "The tableau file is [here](./task1.twb)\n",
    "\n",
    "## 4.Walk us through what you’ve built, and tell us what type of patterns have you observed within the data (including any anti-patterns or issues)\n",
    "### 4.1 What did you consider when designing your data visualisations?\n",
    "Tried to limit the number of visualizations to one dashboard. Top part assigned to transaction information and bottom plots to customer demographics. It is also interactive allowing other teams to investigate without needing to modify the dashboard or make it overly complicated.\n",
    "\n",
    "As a summury of the main insights:\n",
    "* Most of our transactions are in HKD, I've integrated the USD ammount at a rate of 7.75 HKD/USD. No mayor difference.\n",
    "* The customers are relatively young. (average age of 32 years old) and not particulary skewed towards either female or male\n",
    "* Since we have negative incomes I would assume that the income is self reported or it is calculated from the monthly balance change that did not come from payments. The median (reported?) income is 34K\n",
    "* The top categories by total purchase amount are __travel agencies__ and __rent__ taking 31% and 27% repectively. They are followed by __other__, __Eating Places__, __Shopping__ and lastly with just 1.2% __Grocery__. \n",
    "* If we look at the Volume agains the Total purchase amount, we see that even though the Travel and rent are similar in total amount, rent has a much lower volume of activities (as one would expect). Similarly Eating places and shopping have are much more commong but do not contribute as much to the total amount.\n",
    "* It is intersting to see that there is not seasonality for any of the categories throughout the month. or even when looking at the hour or day of the week. I would have expected Restaurants to rise during lunch/dinner hours or rent to peak at the end of the month. But we do not see this. Purchases happen 24/7.\n",
    "\n",
    "### 4.2 Why did you choose this particular chart to tell the story?\n",
    "I believe the area plot gives a good sense of the distribution along the categories of the payment amounts. The line and bar plot is a common plot and it is easy to relate to. It provides key information about trend and seasonality both for the total amounts and the volume of transactions. \n",
    "\n",
    "Age piramids are a widely used representation to understand the two key factors of the members demographics: age, gender. An income histogram is an easy way to understand the affluence of the customers which is the last piece of basic information not covered by the other visualizations\n",
    "\n",
    "### 4.3 What could go wrong for users who use your chart to interpret the data? How would you improve it?\n",
    "A key problem in this visualization is that the demographics are not per unique member. If a 30 year old engages 5 times more than a 60 year old then the plots will show this. A possibility is to generate a separate dashboard that only deals with the customer data looking at the member base regardless on whether they are purchasing or not. \n",
    "\n",
    "One of the things I would improve would be the number of statistics shown like totals, fractions means and such. On one hand this a good way to comunicate to other teams but it can also become a bit overwhelming, I would ask which KPI's or calculations are they confortable using and modify the dashboard to display them. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 of 4\n",
    "\n",
    "Consider this recent study on VBs in Hong Kong from [Vpon.com](https://www.vpon.com/wp-content/uploads/The_New_Wave_of_Tech_Behemoths_in_Hong_Kong_EN.pdf?utm_source=VBHK_EN) and using the same dataset from\n",
    "Task 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. By utilising the demographic information on this report, how would you propose to use this data to optimise our acquisition funnel?\n",
    "\n",
    "I would begin by comparing our member base to the one described in the report. For example:\n",
    "* The gender distribution is completly different. Is is it beacause we are percieved differently? \n",
    "* Our data has a very large amount of trx on Hotels and travel agencies while the report mentions very little about it.\n",
    "\n",
    "The purpose would be to give feedback to the team of the profile of our members compared to what the report considers to be \"standard\" to make sure that the people we are acquiring are in line with the company's strategy.\n",
    "\n",
    "Secondly I would start to look at the descriptions of the transactions in detail. and See what seems to be trending, and check if there is any area we might be missing like Toys games or electronics. \n",
    "\n",
    "The main objective of this is to understand our member base in contrast to the \"baseline\" presented in the report to effectively personalize for the acquisition funnel. A very important part of this personalization would be a customer segmentaion or better still a persona analysis, a comprehensive definition of MOX's poster child customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How would you propose to segment the users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would depend on the exact case (is it for a specific campaing or strategy, is it a general persona, is it focused on customer value) and if would have to be done with other teams like customer insight team and strategy teams. But I would start by generating a \"member model\", a series of aggregations of features for each member. For example: \n",
    "* Demographic features: Age, gender, income\n",
    "* Behavioral features:\n",
    "    * Total money spend by category, frequency and recency of activity.\n",
    "    * When is the money spend (based in the 01:00 to 04:00 activity seen in the report)\n",
    "    * Try to massage a bit more the descriptions (NLP) to generate subcategories like the ones in the report: Games, Dating, Car etc.\n",
    "    \n",
    "I would then proceed to analyse the data with some dimensionality reduction technique like PCA or NMF and try to understand what are the main differences between our customers. I would keep in mind the report to guide the process to eliminate \"noise\" and weight appropiatly the information in this process, e.g. maybe the total amount ($) is not as important as the kind of purchase.\n",
    "\n",
    "After this I would try several clustering approaches (KMeans or Hierarchical clustering to begin with). After a satisfactory clustering I would need to check with and insights team or strategy team to determine which ones are relevant which ones can be grouped toguether and add the qualitative side to the quantitative, so that the segmentation is align with any acquisition strategy in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "Tasked is answered in the pdf task3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4\n",
    "I will create random example tables for the clase presented in the task.\n",
    "\n",
    "We will create a df_prod table which contains the price and cost of a specific product (SKU). We will generate 100 SKU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_skus = 100 # number of products\n",
    "prods = []\n",
    "for i in range(n_skus): \n",
    "    sku = i + 100  # Start giving numbers from 100\n",
    "    price = random.randint(20, 200)  # Each price will be a random number between 20 and 200 ($)\n",
    "    cost = round(price * (0.7 + random.random()*0.29)) # The cost will be random between 70% and 99% of the price\n",
    "    prods.append([sku, price, cost])\n",
    "df_prod = pd.DataFrame(prods, columns=['sku', 'price', 'cost']).set_index('sku')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sku</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>70</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>65</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>159</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price  cost\n",
       "sku             \n",
       "100     70    59\n",
       "101     65    52\n",
       "102    159   138\n",
       "103     60    50\n",
       "104     31    23"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create a sales table, it will contain what product was sold when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = 10000  # Lets create 10K sales\n",
    "sales = []\n",
    "for _ in range(total_sales):\n",
    "    sku = df_prod.sample(1).index[0]  # Pick a random product\n",
    "    year = random.randint(2019,2020)  # Randomly 2019, 2020\n",
    "    month = random.randint(1,12)  # Any month\n",
    "    day = random.randint(1, monthrange(year, month)[1])  # Any day of that month\n",
    "    sales.append([sku, datetime.date(year, month, day)])  \n",
    "df_sales = pd.DataFrame(sales, columns=['sku', 'date'])\n",
    "df_sales['date'] = pd.to_datetime(df_sales['date'])  # Parse the date to datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this data we are now asked the following:\n",
    ">A shopkeeper wants to know which combination of items he should stock in order to produce the\n",
    ">highest profits. Produce a solution which presents the best selling products by volume and\n",
    ">profitability, per day, week, month, year, and overall.\n",
    "\n",
    "I am not being asked how to produce the highest profits, but to show the best selling products. The calculation procedure would be quite straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join sales with product information\n",
    "df_join = df_sales.join(df_prod, on='sku', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the profit from each sale and create convenient time features to filter and group by\n",
    "df_join['profitability'] = df_join.price - df_join.cost \n",
    "df_join['year'] = df_join.date.dt.year\n",
    "df_join['month'] = df_join.date.dt.month\n",
    "df_join['week'] = df_join.date.dt.isocalendar().week\n",
    "df_join['day'] = df_join.date.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019</th>\n",
       "      <th>1</th>\n",
       "      <td>156.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>104.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "sku           100   101   102   103    104    105   106   107   108   109  \\\n",
       "year month                                                                  \n",
       "2019 1      156.0  36.0  12.0  33.0  360.0   66.0  14.0  42.0  42.0  24.0   \n",
       "     2      104.0  18.0  15.0  55.0  135.0   44.0  70.0  12.0  35.0  36.0   \n",
       "     3       78.0  36.0  12.0  33.0  270.0  110.0  42.0  12.0  21.0  48.0   \n",
       "     4       78.0  12.0  21.0  77.0  135.0   66.0  14.0  12.0  56.0  48.0   \n",
       "     5      104.0  24.0  15.0  33.0  225.0  154.0  84.0  18.0  35.0  84.0   \n",
       "\n",
       "sku         ...    190    191   192   193   194   195    196    197    198  \\\n",
       "year month  ...                                                              \n",
       "2019 1      ...   26.0  148.0  60.0  64.0  10.0   6.0  230.0   20.0  175.0   \n",
       "     2      ...   26.0  185.0  80.0  24.0   6.0   6.0    NaN  120.0  420.0   \n",
       "     3      ...   78.0  185.0  40.0  16.0  10.0  12.0  276.0   80.0  140.0   \n",
       "     4      ...  104.0   74.0  60.0  48.0   8.0   3.0  138.0   80.0  105.0   \n",
       "     5      ...   26.0  111.0  60.0  32.0   6.0  24.0  230.0  120.0  140.0   \n",
       "\n",
       "sku         199  \n",
       "year month       \n",
       "2019 1      3.0  \n",
       "     2      4.0  \n",
       "     3      3.0  \n",
       "     4      4.0  \n",
       "     5      3.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we pivot on the desired time frames (e.g. year-month) and the product (sku) and then sum the profitability\n",
    "# We will get a table that for each year-month has the profit generated by each product due to volume and product\n",
    "# profitability\n",
    "profit_table = pd.pivot_table(df_join,\n",
    "               values='profitability',\n",
    "               index=['year', 'month'], \n",
    "               columns=['sku'], \n",
    "               aggfunc = 'sum')\n",
    "profit_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year  month\n",
       "2019  1        [104, 183, 196, 182, 171]\n",
       "      2        [198, 188, 161, 114, 132]\n",
       "      3        [161, 196, 104, 115, 132]\n",
       "      4        [132, 183, 120, 126, 171]\n",
       "      5        [132, 171, 196, 104, 173]\n",
       "dtype: object"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now only need to capture the top k (e.g. 5) products for each time frame\n",
    "# This can be done applying a lambda function to each row like below\n",
    "profit_table.apply(lambda s: s.nlargest(5).index.tolist(), axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Jan 2019 product 104 was the most profitable, and may 2019 product 132."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline would have to be delivered either through a dashboard or an application.\n",
    "Depending on how the shopkeper would like to recieve this information.\n",
    "We would probably optimize the problem if the request is for an specific date so that we can save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
